# -*- coding: utf-8 -*-
"""heart failure prediction dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14jdmw2mfiRDyebvKd_AQDENoo3aTMTkq
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import missingno as msno

import warnings
warnings.filterwarnings('ignore')

sns.set()
plt.style.use('ggplot')

import pandas as pd
from google.colab import files

# Upload the file
uploaded = files.upload()

# Read the CSV file into a DataFrame
df = pd.read_csv(list(uploaded.keys())[0])

# Display the first few rows of the DataFrame
df.head()

import pandas as pd
from google.colab import files

# Upload the file
uploaded = files.upload()

# Read the CSV file into a DataFrame
df = pd.read_csv(list(uploaded.keys())[0])

# Display the column names
print("Column names:", df.columns)

# Display the first few rows of the DataFrame
print("First few rows of the DataFrame:")
print(df.head())

# Replace 'diagnosis' with the correct column name once identified
# For example, if the correct column name is 'HeartDisease':
# unique_values = df['HeartDisease'].unique()
# print("Unique values in 'HeartDisease':", unique_values)

df.info()

df.describe()

y=df['HeartDisease']
plot_sb = sns.countplot(df,x=y, label='Total')
Rain, NotRain =y.value_counts()
print('Have Heart Disease: ',Rain)
print('Not Have Heart Disease : ',NotRain)

df.hist(figsize=(12, 10))
plt.suptitle('Histograma for number of categoricals')
plt.show()

df.hist(figsize=(12, 10))
plt.suptitle('Histograma for number of categoricals')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid")

!pip install xgboost catboost lightgbm

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier

from sklearn.metrics import confusion_matrix, classification_report

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('heart.csv')

print(df.columns)

# Identify categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns

# Apply OneHotEncoder to categorical columns
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), categorical_cols)
    ],
    remainder='passthrough'
)

# Apply preprocessing
X = df.drop('HeartDisease', axis=1)  # Replace 'HeartDisease' with the actual target column name if different
y = df['HeartDisease']  # Replace 'HeartDisease' with the actual target column name if different

X = preprocessor.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid")

!pip install xgboost catboost lightgbm

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier

from sklearn.metrics import confusion_matrix, classification_report

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('heart.csv')

print(df.columns)

# Define LabelEncoders for categorical columns
sex = LabelEncoder()
cpt = LabelEncoder()
recg = LabelEncoder()
ea = LabelEncoder()
st = LabelEncoder()

# Encode categorical columns
df['Sex'] = sex.fit_transform(df['Sex'])
df['ChestPainType'] = cpt.fit_transform(df['ChestPainType'])
df['RestingECG'] = recg.fit_transform(df['RestingECG'])
df['ExerciseAngina'] = ea.fit_transform(df['ExerciseAngina'])
df['ST_Slope'] = st.fit_transform(df['ST_Slope'])

# Identify categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns

# Apply OneHotEncoder to categorical columns
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(), categorical_cols)
    ],
    remainder='passthrough'
)

# Apply preprocessing
X = df.drop('HeartDisease', axis=1)  # Replace 'HeartDisease' with the actual target column name if different
y = df['HeartDisease']  # Replace 'HeartDisease' with the actual target column name if different

X = preprocessor.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

df.head()

corr_matrix=df.corr(method='pearson')
plt.figure(figsize=(10,10))
sns.heatmap(corr_matrix,annot=True,cmap='coolwarm')

thresh_hold=0.2
select_feat=corr_matrix.index[abs(corr_matrix['HeartDisease'])>=thresh_hold].to_list()
select_feat.remove('HeartDisease')
print(select_feat)

selected_features=df[select_feat]
selected_features.head()

target=df['HeartDisease']
target

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid")

!pip install xgboost catboost lightgbm

from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier

from sklearn.metrics import confusion_matrix, classification_report

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('heart.csv')

print(df.columns)

# Define LabelEncoders for categorical columns
sex = LabelEncoder()
cpt = LabelEncoder()
recg = LabelEncoder()
ea = LabelEncoder()
st = LabelEncoder()

# Encode categorical columns
df['Sex'] = sex.fit_transform(df['Sex'])
df['ChestPainType'] = cpt.fit_transform(df['ChestPainType'])
df['RestingECG'] = recg.fit_transform(df['RestingECG'])
df['ExerciseAngina'] = ea.fit_transform(df['ExerciseAngina'])
df['ST_Slope'] = st.fit_transform(df['ST_Slope'])

# Separate features and target
X = df.drop('HeartDisease', axis=1)  # Replace 'HeartDisease' with the actual target column name if different
y = df['HeartDisease']  # Replace 'HeartDisease' with the actual target column name if different

# Define selected features (for the example, we use all features; adjust as needed)
selected_features = X

# Scale the features
scaler = StandardScaler()
selected_features = scaler.fit_transform(selected_features)

print(selected_features)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.2, random_state=42)

# Train the model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

x_train,x_test,y_train,y_test=train_test_split(selected_features,target,test_size=0.2)

from sklearn.linear_model import LogisticRegression

LOG_model=LogisticRegression()
LOG_model.fit(x_train,y_train)

log_pred=LOG_model.predict(x_test)

from sklearn.metrics import accuracy_score  # Import the missing function

accuracy = accuracy_score(y_test, log_pred)

print("Accuracy=", int(accuracy * 100), '%')

from xgboost import XGBClassifier

XGB_model=XGBClassifier()
XGB_model.fit(x_train,y_train)

xgb_pred=XGB_model.predict(x_test)

accuracy=accuracy_score(y_test,xgb_pred)

print("Accuracy=",int(accuracy*100),'%')

from sklearn.svm import SVC

SVM_model=SVC()
SVM_model.fit(x_train,y_train)

svm_pred=SVM_model.predict(x_test)

accuracy=accuracy_score(y_test,svm_pred)

print("Accuracy=",int(accuracy*100),'%')

"""from sklearn.neighbors import KNeighborsClassifier

KNN_model=KNeighborsClassifier(n_neighbors=5)
KNN_model.fit(x_train,y_train)
"""

knn_pred=KNN_model.predict(x_test)

accuracy=accuracy_score(y_test,knn_pred)

print("Accuracy=",int(accuracy*100),'%')

from sklearn.tree import DecisionTreeClassifier

DTree_model=DecisionTreeClassifier()
DTree_model.fit(x_train,y_train)

dtree_pred=DTree_model.predict(x_test)

accuracy=accuracy_score(y_test,dtree_pred)

print("Accuracy=",int(accuracy*100),'%')

import tensorflow as tf
from tensorflow import keras

model=keras.Sequential([keras.layers.Dense(100,activation="relu"), keras.layers.Dense(2,activation="sigmoid")])

model.compile(optimizer="adam",
              loss="sparse_categorical_crossentropy"
              ,metrics=['accuracy'])

model.fit(x_train,y_train,epochs=10)

test_loss, test_acc = model.evaluate(x_test, y_test)

print('Accuracy =',int(test_acc*100),'%')